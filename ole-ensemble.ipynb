{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/jovyan/work/stream-learn\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from stream-learn==0.7.1) (1.17.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from stream-learn==0.7.1) (1.3.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from stream-learn==0.7.1) (0.21.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from stream-learn==0.7.1) (4.32.2)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from stream-learn==0.7.1) (0.18.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->stream-learn==0.7.1) (0.13.2)\n",
      "Installing collected packages: stream-learn\n",
      "  Found existing installation: stream-learn 0.7.1\n",
      "    Uninstalling stream-learn-0.7.1:\n",
      "      Successfully uninstalled stream-learn-0.7.1\n",
      "  Running setup.py develop for stream-learn\n",
      "Successfully installed stream-learn\n"
     ]
    }
   ],
   "source": [
    "!pip install -e ./stream-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.7/site-packages (19.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strlearn.streams import StreamGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = StreamGenerator(n_classes=2, n_drifts=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from attr import attrs, attrib, Factory\n",
    "from copy import deepcopy\n",
    "from typing import List, Callable, NewType, Any, Optional, Dict\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classifier = NewType('Classifier', Any)\n",
    "Instance = NewType('Instance', Any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OALELabelingStrategy(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def label(self, x: Instance): raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "@attrs(auto_attribs=True)\n",
    "class RandomStrategy(OALELabelingStrategy):\n",
    "    _threshold_adjustment_step: int = attrib(default=0.1)\n",
    "        \n",
    "    def label(self, x: Instance):\n",
    "        if self._threshold_adjustment_step <= np.random.uniform():\n",
    "            return True\n",
    "        return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "@attrs(auto_attribs=True)\n",
    "class UncertaintyStrategy(OALELabelingStrategy):\n",
    "    _threshold_margin:int = attrib()\n",
    "    _threshold_adjustment_step: int = attrib()\n",
    "        \n",
    "    def label(self, x: Instance):\n",
    "        return True # TODO(bgulowaty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "@attrs(auto_attribs=True)\n",
    "class CompositeStrategy(OALELabelingStrategy):\n",
    "    _main_strategy: OALELabelingStrategy\n",
    "    _fallback_strategy: OALELabelingStrategy\n",
    "        \n",
    "    def label(self, x: Instance):\n",
    "        labeling = self._main_strategy.label(x)\n",
    "        \n",
    "        if labeling is not True:\n",
    "            labeling = self._fallback_strategy.label(x)\n",
    "        \n",
    "        return labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def originalPaperLabelingStrategyProvider(uncertainity_strategy_threshold, uncertainity_strategy_margin, random_strategy_parameter):\n",
    "    return RandomStrategy(random_strategy_parameter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scikit_gaussian_nb_provider(x, y, classes):\n",
    "    clf = GaussianNB()\n",
    "    clf.partial_fit(x,  y, classes=classes)\n",
    "    \n",
    "    return clf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "\n",
    "class BaseEnsemblePredictionCombiner(metaclass=ABCMeta):\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, x):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "@attrs\n",
    "class WeightedMajorityPredictionCombiner(BaseEnsemblePredictionCombiner):\n",
    "    _ensemble = attrib()\n",
    "    _weights = attrib()\n",
    "    _classes = attrib()\n",
    "\n",
    "    def predict(self, x):\n",
    "        all_members_can_return_supports = all([hasattr(clf, 'predict_proba') for clf in self._ensemble])\n",
    "\n",
    "        if all_members_can_return_supports:\n",
    "            supports_by_clf = [clf.predict_proba(x) * weight for (weight, clf) in zip(self._weights, self._ensemble)]\n",
    "            supports_sum_by_sample = sum(supports_by_clf)\n",
    "            predictions = [self._classes[idx] for idx in np.argmax(supports_sum_by_sample, axis=1)]\n",
    "\n",
    "        else:\n",
    "            predictions_by_clf = [clf.predict(x) for clf in self._ensemble]\n",
    "            supports_by_clf = [\n",
    "                np.vstack(\n",
    "                    [(predictions == clazz).T * weight for clazz in self._classes]\n",
    "                ) for (weight, predictions) in zip(self._weights, predictions_by_clf)\n",
    "            ]\n",
    "\n",
    "            supports_sum_by_sample = sum(supports_by_clf)\n",
    "\n",
    "            predictions = [self._classes[idx] for idx in np.argmax(supports_sum_by_sample, axis=0)]\n",
    "    \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "@attrs(auto_attribs=True)\n",
    "class OALE(BaseEstimator, ClassifierMixin):\n",
    "    _classifier_provider: Callable[[List[Any], List[Any], List[Any]], Classifier] = scikit_gaussian_nb_provider\n",
    "    _block_size: int = 30 # I\n",
    "    _dynamics_clfs_limit: int = 9 # D\n",
    "    _initial_selection_ratio: int = 0.5 # r\n",
    "    _threshold_adjustment_step: int = attrib(default=0.5) # s\n",
    "    _margin_threshold: int = attrib(default=1) # theta\n",
    "    _labeling_strategy_provider: Callable[..., OALELabelingStrategy] = originalPaperLabelingStrategyProvider\n",
    "\n",
    "    \n",
    "#     _ensemble: List = attrib(factory=list, init=False)\n",
    "    _classes: List = attrib(factory=list, init=False)\n",
    "    \n",
    "    _cache: List = attrib(factory=list, init=False) # of tuples (x,y)\n",
    "    _processed_instances: int = attrib(default=0, init=False)\n",
    "    _dynamic_clfs_count: int = attrib(default=0, init=False)\n",
    "    _theta_m: int = attrib(default=0, init=False)\n",
    "    \n",
    "    _stable_clf: Optional[Classifier] = attrib(default=None, init=False)\n",
    "    _stable_clf_weight: int = attrib(init=False, default=0.5)\n",
    "    _dynamic_clfs_weights: List[int] = attrib(init=False)\n",
    "    _dynamic_clfs: List[Classifier] = attrib(factory=list, init=False)\n",
    "    \n",
    "    @_dynamic_clfs_weights.default\n",
    "    def init_dynamic_clfs_weights(self):\n",
    "        return np.zeros(self._dynamics_clfs_limit + 1)\n",
    "    \n",
    "    ## SINGLE INSTANCE\n",
    "    \n",
    "    def partial_fit(self, X, y, classes=None):\n",
    "        X, y = check_X_y(X, y)\n",
    "        \n",
    "        for x_single, y_single in zip(X, y):\n",
    "            self._partial_fit(x_single, y_single, classes)\n",
    "        \n",
    "    def _partial_fit(self, x_new, y_new, classes=None):\n",
    "            \n",
    "        self._classes = classes\n",
    "        if self._classes is None:\n",
    "            self._classes, _ = np.unique(y, return_inverse=True)\n",
    "        \n",
    "        new_instance = (x_new, y_new)\n",
    "        self._processed_instances += 1\n",
    "        \n",
    "        if self._processed_instances < self._block_size: # fill the circular array for the first time\n",
    "            self._cache.append(new_instance)\n",
    "            \n",
    "        elif self._processed_instances == self._block_size: # the first fill of the circular array\n",
    "            self._cache.append(new_instance)\n",
    "            new_clf = self._create_new_classifier()\n",
    "            self._dynamic_clfs_count = 1\n",
    "            self._stable_clf = new_clf # create C_s\n",
    "            self._dynamic_clfs = np.append(self._dynamic_clfs, deepcopy(new_clf)) #  create the first dynamic classifier\n",
    "        else: # more instances processed than block size\n",
    "            i = (self._processed_instances - 1)%self._block_size # i is the current index for a\n",
    "            self._deal_instance(new_instance, i)\n",
    "            i = (i + 1)%self._block_size  # i moves circularly\n",
    "            if i == 0:  # new instances fill A again\n",
    "                self._dynamic_clfs_count += 1\n",
    "                new_clf = self._create_new_classifier()\n",
    "                self._dynamic_clfs = np.append(self._dynamic_clfs, new_clf)\n",
    "                self._theta_m = self._margin_threshold * 2/len(self._classes) # reset theta_m for UncertaintyStrategy\n",
    "                if self._dynamic_clfs_count > self._dynamics_clfs_limit: \n",
    "                    self._dynamic_clfs = np.delete(self._dynamic_clfs, 0)\n",
    "                self._update_weights()\n",
    "        \n",
    "#         for i in range(0, self._block_size - 1): # the array still have I instances to deal\n",
    "#             print(f\"iteration {i} block size {self._block_size}\")\n",
    "#             print(f\"cache size {len(self._cache)}\")\n",
    "#             x = self._cache[i]\n",
    "#             self._deal_instance(x, i)\n",
    "\n",
    "        return self\n",
    "            \n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = check_array(x)\n",
    "        print(x)\n",
    "        preds = WeightedMajorityPredictionCombiner(\n",
    "            ensemble=np.concatenate(([self._stable_clf], self._dynamic_clfs)),\n",
    "            weights=np.concatenate(([self._stable_clf_weight], self._dynamic_clfs_weights)),\n",
    "            classes=self._classes)\\\n",
    "        .predict(x)\n",
    "        \n",
    "        print(preds)\n",
    "        \n",
    "        return preds\n",
    "                \n",
    "    def _update_classifier(self, x, y, clf):\n",
    "        y = [y] if not isinstance(y, np.ndarray) else y\n",
    "        x = np.array(x)\n",
    "        x = np.array([x]) if x.ndim == 1 else x\n",
    "        try:\n",
    "            clf.partial_fit(x, y, self._classes)\n",
    "        except Exception as e:\n",
    "            raise BaseClassifierDoesNotSupportPartialFitting(e)\n",
    "        \n",
    "    def _get_randomly_chosen_instances_to_label(self):\n",
    "        instances_to_label_count = int(self._initial_selection_ratio * len(self._cache))\n",
    "        random_idxs = np.random.choice(len(self._cache), instances_to_label_count, replace=False)\n",
    "        \n",
    "        print(len(self._cache))\n",
    "        print(random_idxs)\n",
    "        print(self._cache[0])\n",
    "        print(instances_to_label_count)\n",
    "        \n",
    "        return np.take(self._cache, random_idxs, axis=0)\n",
    "        \n",
    "    def _create_new_classifier(self):\n",
    "        instances = self._get_randomly_chosen_instances_to_label()\n",
    "        x = np.stack(instances[:, 0]) # TODO(bgulowaty): make this more elegant\n",
    "        y = np.stack(instances[:, 1])\n",
    "        if self._stable_clf != None:\n",
    "            self._update_stable_classifier(x, y)\n",
    "        \n",
    "        return self._classifier_provider(x, y, self._classes)\n",
    "        \n",
    "        \n",
    "    def _update_weights(self):\n",
    "        self._stable_clf_weight = 0.5\n",
    "        self._dynamic_clfs_weights = [current_weight * (1 - 1/self._dynamics_clfs_limit)\n",
    "                                                        for current_weight in self._dynamic_clfs_weights]\n",
    "        self._dynamic_clfs_weights[self._dynamics_clfs_limit] = 1/self._dynamics_clfs_limit\n",
    "    \n",
    "    \n",
    "    def _update_stable_classifier(self, x, y):\n",
    "        self._update_classifier(x, y, self._stable_clf)\n",
    "            \n",
    "    def _get_randomly_selected_cache_instances(self):\n",
    "        random_instances_count = np.ceil(len(self._cache) * self._selection_ratio)\n",
    "        instances_with_labels = np.random.choice(self._cache, random_instances_count)\n",
    "        \n",
    "        return map(list, zip(*instances_with_labels))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _update_dynamic_classifiers(self, x, y):\n",
    "        for clf in self._dynamic_clfs:\n",
    "            self._update_classifier(x, y, clf)\n",
    "    \n",
    "    \n",
    "    def _deal_instance(self, new_instance, i):\n",
    "        x, y = self._cache[i]\n",
    "        \n",
    "        ## TODO(bgulowaty)\n",
    "        labeling_strategy = self._labeling_strategy_provider(0, 0, 0.5)\n",
    "        \n",
    "        labeling = labeling_strategy.label(x)\n",
    "        \n",
    "        if labeling is True:\n",
    "            self._update_stable_classifier(x, y)\n",
    "            self._update_dynamic_classifiers(x, y)\n",
    "        \n",
    "        self._cache[i] = new_instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strlearn.evaluators import PrequentialEvaluator, TestThenTrainEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = GaussianNB()\n",
    "nb.fit([x], [y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/naive_bayes.py:434: RuntimeWarning: divide by zero encountered in log\n",
      "  n_ij = - 0.5 * np.sum(np.log(2. * np.pi * self.sigma_[i, :]))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/naive_bayes.py:436: RuntimeWarning: invalid value encountered in true_divide\n",
      "  (self.sigma_[i, :]), 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.predict([x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ens.predict([x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "[13  9  0 19 10  7  1 16 18 15]\n",
      "(array([ 0.44323356, -0.22243235, -0.41011134, -0.7736701 ,  0.24789053,\n",
      "       -0.55985413,  0.08614155, -1.39403497, -0.6323416 ,  0.16892831,\n",
      "        1.57063048, -1.79853276,  0.75129384, -2.01594175, -0.29787713,\n",
      "        0.37813792,  0.05318888, -0.24636633,  0.85935331,  0.87380524]), 1)\n",
      "10\n",
      "[[-0.52591772  0.33622082  2.17941229 -0.39876838  0.86046269 -0.70450029\n",
      "   0.60056307 -0.29628586 -0.9647923  -0.07036195 -0.993891    0.77682733\n",
      "  -0.85160291  0.62332828 -1.29236639 -0.13992957 -0.4938495  -0.85444435\n",
      "  -0.60158826 -0.4464835 ]]\n",
      "[0]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-529-5b70353d5de4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/work/stream-learn/strlearn/evaluators/TestThenTrainEvaluator.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, stream, clfs, metrics)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     self.scores_[clfid, stream.chunk_id - 1] = [\n\u001b[0;32m---> 84\u001b[0;31m                         \u001b[0mmetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                     ]\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/stream-learn/strlearn/evaluators/TestThenTrainEvaluator.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     self.scores_[clfid, stream.chunk_id - 1] = [\n\u001b[0;32m---> 84\u001b[0;31m                         \u001b[0mmetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                     ]\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/stream-learn/strlearn/utils/metrics.py\u001b[0m in \u001b[0;36mbac\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mTP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0mFP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "evaluator = TestThenTrainEvaluator()\n",
    "stream = StreamGenerator(chunk_size=1, n_chunks=20000)\n",
    "first_fit_size = 20\n",
    "clf = OALE(block_size=first_fit_size)\n",
    "for i in range(first_fit_size):\n",
    "    x, y = stream.get_chunk()\n",
    "    clf.partial_fit(x, y, classes=[0, 1])\n",
    "evaluator.process(stream, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.88      , 0.88141026],\n",
       "        [0.88      , 0.88461538],\n",
       "        [0.88      , 0.87820513],\n",
       "        ...,\n",
       "        [0.88      , 0.87820513],\n",
       "        [0.92      , 0.91883117],\n",
       "        [0.88      , 0.87337662]]])"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ens.predict([x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens = OALE()\n",
    "x, y = stream.get_chunk()\n",
    "for x, y in zip(x, y):\n",
    "    ens.partial_fit(x, y, [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseClassifierDoesNotSupportPartialFitting(Exception):\n",
    "    \"\"\"Provided base classifier does not support partial fitting\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.66357216,  0.4365599 , -0.95318728, ..., -0.1097405 ,\n",
       "          1.15705383,  0.07174972],\n",
       "        [ 1.00545866, -0.48418775, -0.08295778, ...,  1.13516185,\n",
       "          0.52872851,  0.75917814],\n",
       "        [-0.29395326, -0.52416437,  0.43990781, ..., -0.68436744,\n",
       "         -1.13641842,  0.32664697],\n",
       "        ...,\n",
       "        [ 0.67810943,  0.24399256, -0.41654112, ..., -0.39413079,\n",
       "         -1.31152763, -0.52695655],\n",
       "        [-0.96570818, -0.02011439, -1.99643682, ...,  0.34816807,\n",
       "          0.69816285, -0.00396668],\n",
       "        [-2.4647308 , -1.11994915, -0.34874583, ..., -1.05118336,\n",
       "          0.03149365,  0.47387064]]),\n",
       " array([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "        1, 1]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream.get_chunk()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
